---
phase: 03-react-agent-streaming
plan: 04
type: execute
wave: 3
depends_on: ["03-03"]
files_modified:
  - backend/main.py
  - backend/models/schemas.py
autonomous: true

must_haves:
  truths:
    - "POST /api/chat accepts messages array and streams SSE response"
    - "Stream uses AI SDK v6 protocol with correct headers"
    - "Agent reasoning and responses are streamed token-by-token"
    - "Errors in stream are handled gracefully"
    - "POST /api/retrieve still works for raw retrieval (backwards compat)"
  artifacts:
    - path: "backend/main.py"
      provides: "Streaming /api/chat endpoint"
      contains: "StreamingResponse"
    - path: "backend/models/schemas.py"
      provides: "AgentChatRequest schema for messages array"
      exports: ["AgentChatRequest", "MessageItem"]
  key_links:
    - from: "backend/main.py"
      to: "backend/agent/graph.py"
      via: "get_agent_graph for streaming"
      pattern: "from agent import get_agent_graph"
    - from: "backend/main.py"
      to: "backend/streaming/sse.py"
      via: "SSE formatting"
      pattern: "from streaming import"
    - from: "backend/main.py"
      to: "AI SDK useChat"
      via: "x-vercel-ai-ui-message-stream header"
      pattern: "x-vercel-ai-ui-message-stream"
---

<objective>
Create the streaming /api/chat endpoint that bridges LangGraph agent to AI SDK frontend.

Purpose: This endpoint receives chat messages from the frontend's useChat hook, invokes the ReAct agent, and streams the response token-by-token using Server-Sent Events (SSE). The SSE format must match AI SDK v6 protocol for proper frontend rendering.

Output: Updated `backend/main.py` with streaming endpoint, new schema for agent requests
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-react-agent-streaming/03-CONTEXT.md
@.planning/phases/03-react-agent-streaming/03-RESEARCH.md
@.planning/phases/03-react-agent-streaming/03-03-SUMMARY.md (if exists)
@backend/main.py
@backend/models/schemas.py
@backend/agent/graph.py
@backend/streaming/sse.py
@frontend/app/api/chat/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add agent request schema to models</name>
  <files>backend/models/schemas.py</files>
  <action>
Update `backend/models/schemas.py` to add schemas for the agent chat endpoint:

```python
from pydantic import BaseModel
from typing import Optional, Literal

class RetrievalResult(BaseModel):
    """Single retrieval result from RAG system."""
    content: str
    source: str
    score: float
    type: str  # "contact", "event", or "general"

class RetrievalResponse(BaseModel):
    """Response from /api/chat endpoint (Phase 2 - raw retrieval)."""
    query: str
    results: list[RetrievalResult]
    message: Optional[str] = None  # For empty results fallback

class ChatRequest(BaseModel):
    """Request body for legacy /api/chat endpoint (Phase 2)."""
    message: str

class HealthResponse(BaseModel):
    """Response from /health endpoint."""
    status: str
    version: str

# --- Agent schemas (Phase 3) ---

class MessageItem(BaseModel):
    """Single message in conversation history.

    Matches AI SDK v6 message format.
    """
    role: Literal["user", "assistant", "system"]
    content: str

class AgentChatRequest(BaseModel):
    """Request body for streaming agent /api/chat endpoint.

    Accepts messages array matching AI SDK useChat format.
    """
    messages: list[MessageItem]

class AgentChatError(BaseModel):
    """Error response for agent endpoint."""
    error: str
    detail: Optional[str] = None
```
  </action>
  <verify>
```python
python -c "
from models.schemas import AgentChatRequest, MessageItem
req = AgentChatRequest(messages=[
    MessageItem(role='user', content='Hello'),
    MessageItem(role='assistant', content='Hi there!')
])
print(f'Messages: {len(req.messages)}')
print(f'First role: {req.messages[0].role}')
"
```
Should show 2 messages with correct roles.
  </verify>
  <done>
AgentChatRequest schema accepts messages array matching AI SDK format.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create streaming agent endpoint in main.py</name>
  <files>backend/main.py</files>
  <action>
Update `backend/main.py` to add the streaming agent endpoint. The existing Phase 2 endpoint is renamed to /api/retrieve for backwards compatibility.

Replace the entire main.py with:

```python
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager
from pathlib import Path
import logging
import uuid

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.errors import GraphRecursionError

from config import get_settings
from models.schemas import (
    HealthResponse,
    ChatRequest,
    RetrievalResponse,
    RetrievalResult,
    AgentChatRequest,
)
from rag import get_vectorstore, get_hybrid_retriever
from rag.retriever import retrieve_with_scores, deduplicate_results
from rag.chunking import chunk_all_knowledge
from rag.vectorstore import init_vectorstore
from rag.retriever import init_hybrid_retriever
from agent import get_agent_graph, get_recursion_limit
from streaming import format_text_delta, format_done, SSE_HEADERS

settings = get_settings()
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize RAG system on startup."""
    print("Initializing RAG system...")
    knowledge_dir = Path(__file__).parent / "knowledge"

    if knowledge_dir.exists() and any(knowledge_dir.rglob("*.md")):
        # Check if vector store already populated
        vs = get_vectorstore()
        if vs._collection.count() == 0:
            print("Vector store empty, running ingestion...")
            documents = chunk_all_knowledge(knowledge_dir)
            init_vectorstore(documents)
            init_hybrid_retriever(documents)
            print(f"Ingested {len(documents)} chunks")
        else:
            # Still need to init hybrid retriever with documents
            documents = chunk_all_knowledge(knowledge_dir)
            init_hybrid_retriever(documents)
            print(f"Loaded existing vector store with {vs._collection.count()} vectors")
    else:
        print("Warning: No knowledge base found. Run scripts/ingest.py first.")

    # Pre-initialize agent graph (validates API key)
    try:
        get_agent_graph()
        print("Agent graph initialized")
    except Exception as e:
        print(f"Warning: Agent not initialized - {e}")
        print("Set MISTRAL_API_KEY in .env to enable agent features")

    yield

    print("Shutting down...")

app = FastAPI(
    title="Berlin City Chatbot API",
    description="RAG-powered chatbot with ReAct agent for Berlin city information",
    version="0.2.0",
    lifespan=lifespan
)

# CORS middleware for frontend integration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    return HealthResponse(status="healthy", version="0.2.0")


# --- Phase 2 legacy endpoint (raw retrieval) ---

@app.post("/api/retrieve", response_model=RetrievalResponse)
async def retrieve(request: ChatRequest):
    """
    Process a chat message and return relevant knowledge base results.

    This is the Phase 2 retrieval-only endpoint. Use /api/chat for
    the full agent experience with streaming.
    """
    query = request.message.strip()

    if not query:
        raise HTTPException(status_code=400, detail="Message cannot be empty")

    retriever = get_hybrid_retriever()
    if retriever is None:
        return RetrievalResponse(
            query=query,
            results=[],
            message="Knowledge base not initialized. Run scripts/ingest.py first."
        )

    # Retrieve with scores
    raw_results = retrieve_with_scores(query, k=settings.retrieval_k)

    # Deduplicate
    unique_results = deduplicate_results(raw_results)

    if not unique_results:
        return RetrievalResponse(
            query=query,
            results=[],
            message="No relevant information found for your query."
        )

    # Format results
    results = [
        RetrievalResult(
            content=doc.page_content,
            source=doc.metadata.get("attribution", "Unknown"),
            score=round(score, 3),
            type=doc.metadata.get("type", "general")
        )
        for doc, score in unique_results
    ]

    return RetrievalResponse(query=query, results=results)


# --- Phase 3 streaming agent endpoint ---

async def stream_agent_response(messages: list, message_id: str):
    """Stream agent response token-by-token.

    Args:
        messages: List of LangChain message objects
        message_id: Unique ID for the streamed message

    Yields:
        SSE formatted events compatible with AI SDK v6
    """
    graph = get_agent_graph()
    recursion_limit = get_recursion_limit()

    try:
        # Stream with messages mode for token visibility
        # CRITICAL: stream_mode="messages" is required for token-by-token streaming
        async for event in graph.astream(
            {"messages": messages},
            config={"recursion_limit": recursion_limit},
            stream_mode="messages"
        ):
            # event is a tuple: (message_chunk, metadata)
            if isinstance(event, tuple) and len(event) == 2:
                message_chunk, metadata = event

                # Only stream AIMessage content (not tool calls or tool messages)
                if hasattr(message_chunk, "content") and message_chunk.content:
                    # Check if this is a reasoning/thought vs final response
                    # For now, stream all as text-delta
                    # TODO: Distinguish reasoning using metadata.langgraph_node
                    yield format_text_delta(message_chunk.content, message_id)

    except GraphRecursionError:
        logger.warning(f"Agent hit recursion limit ({recursion_limit})")
        yield format_text_delta(
            "\n\nI've reached the maximum number of steps. Please try rephrasing your question.",
            message_id
        )
    except Exception as e:
        logger.error(f"Agent stream error: {e}", exc_info=True)
        yield format_text_delta(
            "\n\nI encountered an error processing your request. Please try again.",
            message_id
        )

    yield format_done()


@app.post("/api/chat")
async def chat_stream(request: AgentChatRequest):
    """
    Streaming chat endpoint with ReAct agent.

    Accepts messages array matching AI SDK format, streams response via SSE.

    The response uses AI SDK v6 stream protocol:
    - text-delta events for streaming content
    - [DONE] marker to signal completion

    Headers include x-vercel-ai-ui-message-stream: v1 for AI SDK compatibility.
    """
    if not request.messages:
        raise HTTPException(status_code=400, detail="Messages array cannot be empty")

    # Convert to LangChain message format
    lc_messages = []
    for msg in request.messages:
        if msg.role == "user":
            lc_messages.append(HumanMessage(content=msg.content))
        elif msg.role == "assistant":
            lc_messages.append(AIMessage(content=msg.content))
        elif msg.role == "system":
            lc_messages.append(SystemMessage(content=msg.content))

    if not lc_messages:
        raise HTTPException(status_code=400, detail="No valid messages found")

    # Generate message ID for this response
    message_id = f"msg-{uuid.uuid4().hex[:8]}"

    # Return streaming response with AI SDK headers
    return StreamingResponse(
        stream_agent_response(lc_messages, message_id),
        media_type="text/event-stream",
        headers=SSE_HEADERS,
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=settings.host, port=settings.port)
```

Key changes:
- Renamed old /api/chat to /api/retrieve (backwards compat)
- New /api/chat accepts messages array and streams SSE
- Uses stream_mode="messages" for token-by-token streaming
- SSE_HEADERS includes required AI SDK header
- Handles GraphRecursionError gracefully
  </action>
  <verify>
```bash
cd backend && source venv/bin/activate

# Test that server starts (will fail on agent if no API key, but endpoint should register)
timeout 5 python -c "
from main import app
from fastapi.testclient import TestClient

client = TestClient(app)

# Health check
resp = client.get('/health')
print(f'Health: {resp.status_code}')

# Check routes exist
routes = [r.path for r in app.routes]
print(f'/api/chat exists: {\"/api/chat\" in routes}')
print(f'/api/retrieve exists: {\"/api/retrieve\" in routes}')
" || echo "Server test completed"
```
Both endpoints registered.
  </verify>
  <done>
Streaming /api/chat endpoint accepts messages array and streams SSE with AI SDK v6 protocol.
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify /api/retrieve backwards compatibility</name>
  <files>backend/main.py</files>
  <action>
Verify the renamed /api/retrieve endpoint still works correctly with the existing ChatRequest schema (single message field).

No file changes needed - this is a verification task to ensure the endpoint rename didn't break backwards compatibility.

Test that:
1. POST /api/retrieve accepts `{"message": "query text"}`
2. Returns RetrievalResponse with results array
3. Handles empty message with 400 error
  </action>
  <verify>
```bash
cd backend && source venv/bin/activate

python -c "
from main import app
from fastapi.testclient import TestClient

client = TestClient(app)

# Test /api/retrieve with valid request
resp = client.post('/api/retrieve', json={'message': 'test query'})
print(f'/api/retrieve status: {resp.status_code}')
print(f'Response has query field: {\"query\" in resp.json()}')
print(f'Response has results field: {\"results\" in resp.json()}')

# Test empty message returns 400
resp_empty = client.post('/api/retrieve', json={'message': ''})
print(f'Empty message status: {resp_empty.status_code}')
"
```
Should show 200 for valid request with query/results fields, 400 for empty message.
  </verify>
  <done>
POST /api/retrieve accepts ChatRequest with message field and returns RetrievalResponse. Backwards compatibility confirmed.
  </done>
</task>

</tasks>

<verification>
From backend/ directory:

```bash
cd backend && source venv/bin/activate

# Verify endpoint schema and routing
python -c "
from main import app
from models.schemas import AgentChatRequest, MessageItem

# Check routes
routes = {r.path: r.methods for r in app.routes if hasattr(r, 'methods')}
print('Routes:', routes.get('/api/chat'), routes.get('/api/retrieve'))

# Check schema
req = AgentChatRequest(messages=[MessageItem(role='user', content='Test')])
print(f'Request valid: {len(req.messages)} messages')

# Check SSE headers imported
from streaming import SSE_HEADERS
print(f'SSE headers: {list(SSE_HEADERS.keys())}')
"
```

All checks pass: routes registered, schema works, headers available.
</verification>

<success_criteria>
- POST /api/chat accepts AgentChatRequest with messages array
- Response is StreamingResponse with text/event-stream media type
- Headers include x-vercel-ai-ui-message-stream: v1
- Old retrieval endpoint moved to /api/retrieve and still works
- Agent graph invoked with stream_mode="messages"
- GraphRecursionError handled with user-friendly message
- Stream errors handled gracefully without crashing
</success_criteria>

<output>
After completion, create `.planning/phases/03-react-agent-streaming/03-04-SUMMARY.md`
</output>
