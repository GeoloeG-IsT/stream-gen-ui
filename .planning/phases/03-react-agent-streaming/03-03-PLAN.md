---
phase: 03-react-agent-streaming
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - backend/agent/graph.py
  - backend/agent/__init__.py
  - backend/config.py
autonomous: true

must_haves:
  truths:
    - "LangGraph state machine implements ReAct cycle"
    - "Agent uses Mistral LLM with streaming=True"
    - "Graph respects recursion_limit for max iterations"
  artifacts:
    - path: "backend/agent/graph.py"
      provides: "LangGraph ReAct agent state machine"
      exports: ["create_agent_graph", "get_agent_graph"]
    - path: "backend/config.py"
      provides: "Agent configuration (model, timeout, iterations)"
      contains: "mistral_model"
  key_links:
    - from: "backend/agent/graph.py"
      to: "backend/agent/state.py"
      via: "uses AgentState"
      pattern: "from agent.state import AgentState"
    - from: "backend/agent/graph.py"
      to: "backend/agent/tools.py"
      via: "uses search_knowledge_base tool"
      pattern: "from agent.tools import"
    - from: "backend/agent/graph.py"
      to: "langchain_mistralai"
      via: "ChatMistralAI with streaming"
      pattern: "ChatMistralAI"
---

<objective>
Create the LangGraph ReAct agent state machine with Mistral LLM and proper iteration control.

Purpose: The agent graph is the core ReAct implementation. It orchestrates Thought-Action-Observation cycles using LangGraph's state machine. The agent decides when to invoke the RAG tool and generates streaming responses.

Output: `backend/agent/graph.py` with compiled LangGraph agent
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-react-agent-streaming/03-CONTEXT.md
@.planning/phases/03-react-agent-streaming/03-RESEARCH.md
@.planning/phases/03-react-agent-streaming/03-01-SUMMARY.md (if exists, for state/tools)
@.planning/phases/03-react-agent-streaming/03-02-SUMMARY.md (if exists, for prompts)
@backend/agent/state.py
@backend/agent/tools.py
@backend/agent/prompts.py
@backend/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add agent configuration to config.py</name>
  <files>backend/config.py</files>
  <action>
Update `backend/config.py` to add agent-specific settings:

```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # Server
    host: str = "0.0.0.0"
    port: int = 8000
    debug: bool = True

    # RAG
    embedding_model: str = "sentence-transformers/all-mpnet-base-v2"
    chroma_persist_dir: str = "./chroma_db"
    collection_name: str = "berlin_city_knowledge"

    # Retrieval
    retrieval_k: int = 10
    bm25_weight: float = 0.2
    semantic_weight: float = 0.8

    # Agent (NEW)
    mistral_model: str = "mistral-large-latest"
    mistral_api_key: str = ""  # Required - set via MISTRAL_API_KEY env var
    agent_max_iterations: int = 5
    agent_timeout_seconds: int = 30
    agent_temperature: float = 0.0  # Deterministic for consistent responses

    # Observability (optional)
    langsmith_api_key: str = ""  # Set via LANGSMITH_API_KEY for tracing
    langsmith_project: str = "berlin-city-chatbot"

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

@lru_cache
def get_settings() -> Settings:
    return Settings()
```

Also update `backend/.env.example` to add:
```
# Mistral AI (required for agent)
MISTRAL_API_KEY=your_mistral_api_key

# LangSmith (optional - for observability)
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_TRACING_V2=true
```
  </action>
  <verify>
```python
python -c "
from config import get_settings
s = get_settings()
print(f'Model: {s.mistral_model}')
print(f'Max iterations: {s.agent_max_iterations}')
print(f'Timeout: {s.agent_timeout_seconds}s')
"
```
Should show default config values.
  </verify>
  <done>
Config includes agent settings for model, iterations, timeout.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create LangGraph ReAct agent graph</name>
  <files>backend/agent/graph.py</files>
  <action>
Create `backend/agent/graph.py`:

```python
"""LangGraph ReAct agent state machine.

Implements the Thought-Action-Observation cycle using LangGraph's StateGraph.
The agent decides when to invoke the RAG tool and streams responses.

Key points from research:
- Use stream_mode="messages" for token-by-token streaming
- Recursion limit = 2 * max_iterations + 1 (LangGraph counts each step)
- Model must have streaming=True for token visibility
"""
import logging
from typing import Literal

from langchain_mistralai import ChatMistralAI
from langchain_core.messages import BaseMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.errors import GraphRecursionError

from config import get_settings
from agent.state import AgentState
from agent.tools import search_knowledge_base
from agent.prompts import get_agent_prompt

logger = logging.getLogger(__name__)

# Singleton graph instance
_agent_graph = None


def create_agent_graph():
    """Create and compile the ReAct agent graph.

    Returns:
        Compiled LangGraph state machine ready for streaming execution.
    """
    settings = get_settings()

    # Initialize Mistral LLM with streaming enabled
    # CRITICAL: streaming=True is required for token-by-token visibility
    llm = ChatMistralAI(
        model=settings.mistral_model,
        api_key=settings.mistral_api_key,
        temperature=settings.agent_temperature,
        streaming=True,  # REQUIRED for token streaming
        timeout=settings.agent_timeout_seconds,
    )

    # Bind tools to the model
    tools = [search_knowledge_base]
    llm_with_tools = llm.bind_tools(tools)

    # Create prompt chain
    prompt = get_agent_prompt()
    agent_chain = prompt | llm_with_tools

    def call_agent(state: AgentState) -> dict:
        """Agent node: invoke LLM with current messages.

        The LLM decides whether to call a tool or respond directly.
        """
        messages = state["messages"]
        response = agent_chain.invoke({"messages": messages})
        return {"messages": [response]}

    def should_continue(state: AgentState) -> Literal["tools", "__end__"]:
        """Route to tools if LLM requested tool call, else end.

        Returns:
            "tools" if last message has tool_calls, "__end__" otherwise
        """
        messages = state["messages"]
        last_message = messages[-1]

        # Check if the LLM wants to call a tool
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            logger.debug(f"Agent calling tools: {[tc['name'] for tc in last_message.tool_calls]}")
            return "tools"

        logger.debug("Agent finished - no more tool calls")
        return END

    # Build the graph
    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("agent", call_agent)
    workflow.add_node("tools", ToolNode(tools))

    # Set entry point
    workflow.set_entry_point("agent")

    # Add edges
    # Agent -> (tools or end)
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            END: END,
        }
    )
    # Tools -> back to agent (for observation)
    workflow.add_edge("tools", "agent")

    # Compile the graph
    graph = workflow.compile()

    logger.info(f"Agent graph compiled with model={settings.mistral_model}, max_iterations={settings.agent_max_iterations}")
    return graph


def get_agent_graph():
    """Get or create the agent graph singleton.

    Returns:
        Compiled LangGraph ready for .astream() calls.
    """
    global _agent_graph
    if _agent_graph is None:
        _agent_graph = create_agent_graph()
    return _agent_graph


def get_recursion_limit() -> int:
    """Calculate recursion limit from max iterations.

    LangGraph counts each step (agent call + tool call) separately.
    Formula: recursion_limit = 2 * max_iterations + 1

    For 5 iterations: 2 * 5 + 1 = 11
    """
    settings = get_settings()
    return 2 * settings.agent_max_iterations + 1
```
  </action>
  <verify>
```python
python -c "
from agent.graph import create_agent_graph, get_recursion_limit
# Just test creation (won't work without API key, but should not error on import)
print(f'Recursion limit: {get_recursion_limit()}')
print('Graph module loads correctly')
"
```
Should print recursion limit of 11 (for 5 iterations).
  </verify>
  <done>
LangGraph ReAct agent graph is created with Mistral LLM, tool binding, and proper recursion limit calculation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update agent __init__.py with all exports</name>
  <files>backend/agent/__init__.py</files>
  <action>
Update `backend/agent/__init__.py` to export all agent components:

```python
"""ReAct agent module for Berlin city chatbot.

This module provides the LangGraph-based ReAct agent that:
- Executes Thought-Action-Observation cycles
- Uses RAG as a tool for knowledge retrieval
- Streams responses with entity markers

Usage:
    from agent import get_agent_graph, get_recursion_limit

    graph = get_agent_graph()
    async for event in graph.astream(
        {"messages": messages},
        config={"recursion_limit": get_recursion_limit()},
        stream_mode="messages"
    ):
        # Process streaming events
"""
from agent.state import AgentState
from agent.tools import search_knowledge_base
from agent.prompts import AGENT_SYSTEM_PROMPT, get_agent_prompt
from agent.graph import create_agent_graph, get_agent_graph, get_recursion_limit

__all__ = [
    "AgentState",
    "search_knowledge_base",
    "AGENT_SYSTEM_PROMPT",
    "get_agent_prompt",
    "create_agent_graph",
    "get_agent_graph",
    "get_recursion_limit",
]
```
  </action>
  <verify>
```python
python -c "
from agent import (
    AgentState,
    search_knowledge_base,
    AGENT_SYSTEM_PROMPT,
    get_agent_graph,
    get_recursion_limit,
)
print('All exports available')
print(f'State keys: {list(AgentState.__annotations__.keys())}')
print(f'Tool name: {search_knowledge_base.name}')
print(f'Recursion limit: {get_recursion_limit()}')
"
```
All imports work without error.
  </verify>
  <done>
Agent module exports all components needed for streaming endpoint integration.
  </done>
</task>

</tasks>

<verification>
From backend/ directory:

```bash
cd backend && source venv/bin/activate
python -c "
from config import get_settings
from agent import AgentState, search_knowledge_base, get_agent_graph, get_recursion_limit

settings = get_settings()
print(f'Model: {settings.mistral_model}')
print(f'Max iterations: {settings.agent_max_iterations}')
print(f'Recursion limit: {get_recursion_limit()}')
print(f'Agent state: {list(AgentState.__annotations__.keys())}')
print(f'RAG tool: {search_knowledge_base.name}')
print('Graph module: OK')
"
```

All values printed correctly, graph module loads without error.
</verification>

<success_criteria>
- backend/agent/graph.py creates LangGraph StateGraph with agent and tools nodes
- Agent uses ChatMistralAI with streaming=True
- Conditional routing sends to tools if tool_calls present, else ends
- Recursion limit calculated correctly (11 for 5 iterations)
- Config updated with mistral_model, agent_max_iterations, agent_timeout_seconds
- All agent components exported from __init__.py
</success_criteria>

<output>
After completion, create `.planning/phases/03-react-agent-streaming/03-03-SUMMARY.md`
</output>
