---
phase: 02-backend-foundation-rag
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - backend/rag/embeddings.py
  - backend/rag/chunking.py
  - backend/rag/vectorstore.py
  - backend/rag/retriever.py
  - backend/scripts/ingest.py
  - backend/main.py
autonomous: true

must_haves:
  truths:
    - "Ingestion script processes all markdown files and populates vector store"
    - "POST /api/chat returns relevant results for sample queries"
    - "Hybrid search combines BM25 and semantic retrieval"
    - "Results include relevance scores and source attribution"
    - "Empty queries return fallback message"
  artifacts:
    - path: "backend/rag/embeddings.py"
      provides: "HuggingFace embeddings wrapper"
      contains: "HuggingFaceEmbeddings"
    - path: "backend/rag/chunking.py"
      provides: "Markdown-aware text splitting"
      contains: "MarkdownHeaderTextSplitter"
    - path: "backend/rag/vectorstore.py"
      provides: "ChromaDB initialization"
      contains: "Chroma"
    - path: "backend/rag/retriever.py"
      provides: "Hybrid search with EnsembleRetriever"
      contains: "EnsembleRetriever"
    - path: "backend/scripts/ingest.py"
      provides: "Knowledge base ingestion"
      contains: "def ingest"
    - path: "backend/main.py"
      provides: "API endpoint /api/chat"
      contains: "/api/chat"
  key_links:
    - from: "backend/main.py"
      to: "backend/rag/retriever.py"
      via: "retriever import and invoke"
      pattern: "retriever\\.invoke"
    - from: "backend/rag/retriever.py"
      to: "backend/rag/vectorstore.py"
      via: "vector store as_retriever"
      pattern: "as_retriever"
    - from: "backend/scripts/ingest.py"
      to: "backend/knowledge/**/*.md"
      via: "file glob and processing"
      pattern: "glob.*\\.md"
---

<objective>
Build the complete RAG pipeline with embeddings, chunking, vector store, hybrid retrieval, and API endpoint.

Purpose: Create the retrieval system that finds relevant knowledge base content for user queries.
Output: Working /api/chat endpoint that returns top 10 hybrid search results with scores and source attribution.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-backend-foundation-rag/02-CONTEXT.md
@.planning/phases/02-backend-foundation-rag/02-RESEARCH.md
@.planning/phases/02-backend-foundation-rag/02-01-SUMMARY.md
@.planning/phases/02-backend-foundation-rag/02-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create RAG pipeline components</name>
  <files>
    backend/rag/__init__.py
    backend/rag/embeddings.py
    backend/rag/chunking.py
    backend/rag/vectorstore.py
    backend/rag/retriever.py
  </files>
  <action>
Create the RAG module components following research patterns.

**backend/rag/__init__.py:**
```python
from .embeddings import get_embeddings
from .chunking import chunk_markdown_file, chunk_all_knowledge
from .vectorstore import get_vectorstore, init_vectorstore
from .retriever import get_hybrid_retriever

__all__ = [
    "get_embeddings",
    "chunk_markdown_file",
    "chunk_all_knowledge",
    "get_vectorstore",
    "init_vectorstore",
    "get_hybrid_retriever",
]
```

**backend/rag/embeddings.py:**
```python
from functools import lru_cache
from langchain_huggingface import HuggingFaceEmbeddings
import sys
sys.path.insert(0, '..')
from config import get_settings

@lru_cache
def get_embeddings() -> HuggingFaceEmbeddings:
    """Get cached HuggingFace embeddings instance."""
    settings = get_settings()
    return HuggingFaceEmbeddings(
        model_name=settings.embedding_model,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
```

**backend/rag/chunking.py:**
```python
from pathlib import Path
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_core.documents import Document

HEADERS_TO_SPLIT = [
    ("#", "Department"),
    ("##", "Section"),
    ("###", "Entry"),
]

def chunk_markdown_file(file_path: Path) -> list[Document]:
    """Chunk a markdown file preserving header structure."""
    content = file_path.read_text(encoding="utf-8")

    # Determine document type from path
    if "contacts" in str(file_path):
        doc_type = "contact"
    elif "events" in str(file_path):
        doc_type = "event"
    else:
        doc_type = "general"

    # Extract title from filename
    title = file_path.stem.replace("-", " ").title()

    # First pass: split by headers
    md_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=HEADERS_TO_SPLIT,
        strip_headers=False
    )
    header_splits = md_splitter.split_text(content)

    # Second pass: enforce size limits
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=50
    )

    final_docs = []
    for doc in header_splits:
        # Build attribution from metadata
        parts = [title]
        if "Section" in doc.metadata:
            parts.append(doc.metadata["Section"])
        if "Entry" in doc.metadata:
            parts.append(doc.metadata["Entry"])
        attribution = " > ".join(parts)

        # Apply size splitting if needed
        if len(doc.page_content) > 512:
            sub_docs = text_splitter.split_documents([doc])
            for i, sub_doc in enumerate(sub_docs):
                sub_doc.metadata.update({
                    "source": str(file_path),
                    "title": title,
                    "attribution": attribution,
                    "type": doc_type,
                    "chunk_index": i
                })
                final_docs.append(sub_doc)
        else:
            doc.metadata.update({
                "source": str(file_path),
                "title": title,
                "attribution": attribution,
                "type": doc_type
            })
            final_docs.append(doc)

    return final_docs

def chunk_all_knowledge(knowledge_dir: Path) -> list[Document]:
    """Chunk all markdown files in knowledge directory."""
    all_docs = []
    for md_file in knowledge_dir.rglob("*.md"):
        docs = chunk_markdown_file(md_file)
        all_docs.extend(docs)
    return all_docs
```

**backend/rag/vectorstore.py:**
```python
from pathlib import Path
from functools import lru_cache
from langchain_chroma import Chroma
from langchain_core.documents import Document
import sys
sys.path.insert(0, '..')
from config import get_settings
from .embeddings import get_embeddings

_vectorstore = None

def init_vectorstore(documents: list[Document] | None = None) -> Chroma:
    """Initialize ChromaDB vector store, optionally with documents."""
    global _vectorstore
    settings = get_settings()

    persist_dir = Path(settings.chroma_persist_dir)
    persist_dir.mkdir(parents=True, exist_ok=True)

    _vectorstore = Chroma(
        collection_name=settings.collection_name,
        embedding_function=get_embeddings(),
        persist_directory=str(persist_dir),
        collection_metadata={"hnsw:space": "cosine"}
    )

    if documents:
        _vectorstore.add_documents(documents)

    return _vectorstore

def get_vectorstore() -> Chroma:
    """Get or create vector store instance."""
    global _vectorstore
    if _vectorstore is None:
        _vectorstore = init_vectorstore()
    return _vectorstore
```

**backend/rag/retriever.py:**
```python
from functools import lru_cache
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from nltk.tokenize import word_tokenize
import nltk
import sys
sys.path.insert(0, '..')
from config import get_settings
from .vectorstore import get_vectorstore

# Ensure NLTK data is available
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

_hybrid_retriever = None
_documents_cache = None

def init_hybrid_retriever(documents: list[Document]) -> EnsembleRetriever:
    """Initialize hybrid retriever with BM25 + semantic search."""
    global _hybrid_retriever, _documents_cache
    settings = get_settings()

    # BM25 retriever for keyword search
    bm25_retriever = BM25Retriever.from_documents(
        documents,
        k=settings.retrieval_k,
        preprocess_func=word_tokenize
    )

    # Semantic retriever from vector store
    vectorstore = get_vectorstore()
    semantic_retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": settings.retrieval_k}
    )

    # Ensemble with weights [BM25, semantic] = [0.2, 0.8]
    _hybrid_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, semantic_retriever],
        weights=[settings.bm25_weight, settings.semantic_weight]
    )

    _documents_cache = documents
    return _hybrid_retriever

def get_hybrid_retriever() -> EnsembleRetriever | None:
    """Get hybrid retriever instance (must be initialized first)."""
    return _hybrid_retriever

def retrieve_with_scores(query: str, k: int = 10) -> list[tuple[Document, float]]:
    """Retrieve documents with relevance scores."""
    if _hybrid_retriever is None:
        return []

    # EnsembleRetriever doesn't return scores directly
    # Use vector store for scored results
    vectorstore = get_vectorstore()
    results = vectorstore.similarity_search_with_score(query, k=k)

    # Convert distance to similarity (lower distance = higher similarity)
    scored_results = []
    for doc, distance in results:
        relevance = max(0, 1 - distance)  # Clamp to [0, 1]
        scored_results.append((doc, relevance))

    return scored_results

def deduplicate_results(
    results: list[tuple[Document, float]],
    similarity_threshold: float = 0.95
) -> list[tuple[Document, float]]:
    """Remove near-duplicate chunks from same source."""
    from collections import defaultdict

    seen_sources = defaultdict(list)
    unique_results = []

    for doc, score in results:
        source = doc.metadata.get("source", "")
        content = doc.page_content
        content_words = set(content.lower().split())

        is_duplicate = False
        for seen_content in seen_sources[source]:
            seen_words = set(seen_content.lower().split())
            if len(content_words) == 0:
                continue
            overlap = len(content_words & seen_words) / len(content_words)
            if overlap > similarity_threshold:
                is_duplicate = True
                break

        if not is_duplicate:
            seen_sources[source].append(content)
            unique_results.append((doc, score))

    return unique_results
```
  </action>
  <verify>
    - `cd backend && python -c "from rag import get_embeddings; print(type(get_embeddings()))"` shows HuggingFaceEmbeddings
    - `cd backend && python -c "from rag.chunking import HEADERS_TO_SPLIT; print(len(HEADERS_TO_SPLIT))"` shows 3
    - `cd backend && python -c "from rag import get_vectorstore; print(type(get_vectorstore()))"` shows Chroma
    - All imports resolve without errors
  </verify>
  <done>RAG pipeline components created: embeddings wrapper, markdown chunking, ChromaDB vector store, and hybrid retriever with deduplication</done>
</task>

<task type="auto">
  <name>Task 2: Create ingestion script</name>
  <files>
    backend/scripts/ingest.py
  </files>
  <action>
Create the knowledge base ingestion script.

**backend/scripts/ingest.py:**
```python
#!/usr/bin/env python
"""Ingest knowledge base markdown files into vector store."""
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag.chunking import chunk_all_knowledge
from rag.vectorstore import init_vectorstore
from rag.retriever import init_hybrid_retriever

def ingest(knowledge_dir: Path | None = None) -> dict:
    """
    Ingest all knowledge base files into vector store.

    Returns dict with ingestion stats.
    """
    if knowledge_dir is None:
        knowledge_dir = Path(__file__).parent.parent / "knowledge"

    print(f"Ingesting knowledge from: {knowledge_dir}")

    # Chunk all markdown files
    print("Chunking documents...")
    documents = chunk_all_knowledge(knowledge_dir)
    print(f"Created {len(documents)} chunks")

    # Count by type
    type_counts = {}
    for doc in documents:
        doc_type = doc.metadata.get("type", "unknown")
        type_counts[doc_type] = type_counts.get(doc_type, 0) + 1

    print(f"Chunk breakdown: {type_counts}")

    # Initialize vector store with documents
    print("Initializing vector store...")
    vectorstore = init_vectorstore(documents)
    print(f"Vector store initialized with {vectorstore._collection.count()} vectors")

    # Initialize hybrid retriever
    print("Initializing hybrid retriever...")
    init_hybrid_retriever(documents)
    print("Hybrid retriever initialized")

    # Test retrieval
    print("\nTesting retrieval...")
    test_queries = [
        "Who handles emergency services?",
        "When is the next city council meeting?",
        "How do I get a building permit?"
    ]

    from rag.retriever import retrieve_with_scores
    for query in test_queries:
        results = retrieve_with_scores(query, k=3)
        print(f"\nQuery: {query}")
        for doc, score in results[:3]:
            print(f"  [{score:.3f}] {doc.metadata.get('attribution', 'Unknown')}")

    return {
        "total_chunks": len(documents),
        "by_type": type_counts,
        "vector_count": vectorstore._collection.count()
    }

if __name__ == "__main__":
    stats = ingest()
    print(f"\nIngestion complete: {stats}")
```

Make the script executable and test it:
```bash
chmod +x backend/scripts/ingest.py
```
  </action>
  <verify>
    - `ls -la backend/scripts/ingest.py` shows the file exists
    - `cd backend && python scripts/ingest.py` runs without errors (after knowledge base exists)
    - Ingestion prints chunk counts and test query results
  </verify>
  <done>Ingestion script created that chunks all markdown files, populates vector store, initializes hybrid retriever, and runs test queries</done>
</task>

<task type="auto">
  <name>Task 3: Add /api/chat endpoint to FastAPI</name>
  <files>
    backend/main.py
  </files>
  <action>
Update backend/main.py to add the /api/chat endpoint.

**Replace the placeholder comment with working endpoint:**
```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from pathlib import Path

from config import get_settings
from models.schemas import (
    HealthResponse,
    ChatRequest,
    RetrievalResponse,
    RetrievalResult
)
from rag import get_vectorstore, get_hybrid_retriever
from rag.retriever import retrieve_with_scores, deduplicate_results
from rag.chunking import chunk_all_knowledge
from rag.vectorstore import init_vectorstore
from rag.retriever import init_hybrid_retriever

settings = get_settings()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize RAG system on startup."""
    print("Initializing RAG system...")
    knowledge_dir = Path(__file__).parent / "knowledge"

    if knowledge_dir.exists() and any(knowledge_dir.rglob("*.md")):
        # Check if vector store already populated
        vs = get_vectorstore()
        if vs._collection.count() == 0:
            print("Vector store empty, running ingestion...")
            documents = chunk_all_knowledge(knowledge_dir)
            init_vectorstore(documents)
            init_hybrid_retriever(documents)
            print(f"Ingested {len(documents)} chunks")
        else:
            # Still need to init hybrid retriever with documents
            documents = chunk_all_knowledge(knowledge_dir)
            init_hybrid_retriever(documents)
            print(f"Loaded existing vector store with {vs._collection.count()} vectors")
    else:
        print("Warning: No knowledge base found. Run scripts/ingest.py first.")

    yield

    print("Shutting down...")

app = FastAPI(
    title="Berlin City Chatbot API",
    description="RAG-powered chatbot for Berlin city information",
    version="0.1.0",
    lifespan=lifespan
)

# CORS middleware for frontend integration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    return HealthResponse(status="healthy", version="0.1.0")

@app.post("/api/chat", response_model=RetrievalResponse)
async def chat(request: ChatRequest):
    """
    Process a chat message and return relevant knowledge base results.

    This is Phase 2 retrieval-only endpoint. Phase 3 will add the ReAct agent
    that synthesizes responses from these results.
    """
    query = request.message.strip()

    if not query:
        raise HTTPException(status_code=400, detail="Message cannot be empty")

    retriever = get_hybrid_retriever()
    if retriever is None:
        return RetrievalResponse(
            query=query,
            results=[],
            message="Knowledge base not initialized. Run scripts/ingest.py first."
        )

    # Retrieve with scores
    raw_results = retrieve_with_scores(query, k=settings.retrieval_k)

    # Deduplicate
    unique_results = deduplicate_results(raw_results)

    if not unique_results:
        return RetrievalResponse(
            query=query,
            results=[],
            message="No relevant information found for your query."
        )

    # Format results
    results = [
        RetrievalResult(
            content=doc.page_content,
            source=doc.metadata.get("attribution", "Unknown"),
            score=round(score, 3),
            type=doc.metadata.get("type", "general")
        )
        for doc, score in unique_results
    ]

    return RetrievalResponse(query=query, results=results)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=settings.host, port=settings.port)
```
  </action>
  <verify>
    - `cd backend && python -c "from main import app; print([r.path for r in app.routes])"` shows /health and /api/chat
    - Server starts and initializes RAG system
    - `curl -X POST http://localhost:8000/api/chat -H "Content-Type: application/json" -d '{"message":"emergency services"}'` returns results
  </verify>
  <done>/api/chat endpoint added that performs hybrid retrieval, deduplicates results, and returns formatted RetrievalResponse with scores and source attribution</done>
</task>

</tasks>

<verification>
- [ ] All RAG module files exist in backend/rag/
- [ ] Ingestion script runs and populates vector store
- [ ] Vector store persists to disk (chroma_db/ directory)
- [ ] /api/chat endpoint accepts POST requests
- [ ] Query "emergency services" returns relevant contacts
- [ ] Query "city council meeting" returns relevant events
- [ ] Results include relevance scores (0-1 range)
- [ ] Results include source attribution (e.g., "Public Safety > Emergency Services")
- [ ] Empty/no-match queries return fallback message
- [ ] Deduplication removes near-identical chunks
</verification>

<success_criteria>
1. RAG pipeline fully implemented (embeddings, chunking, vectorstore, retriever)
2. Ingestion script processes all knowledge base files
3. /api/chat returns top 10 hybrid search results
4. Results include relevance scores and source attribution
5. Empty results return appropriate fallback message
6. Server auto-initializes RAG system on startup
</success_criteria>

<output>
After completion, create `.planning/phases/02-backend-foundation-rag/02-03-SUMMARY.md`
</output>
