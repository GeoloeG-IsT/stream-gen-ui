---
phase: 05-renderer-integration
plan: 04
type: execute
wave: 3
depends_on: ["05-02", "05-03"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "llm-ui page shows real agent responses"
    - "Streamdown page shows real agent responses"
    - "No raw delimiters/tags visible during streaming on either page"
    - "Stop button works on both pages"
    - "Components fade in smoothly when complete"
  artifacts: []
  key_links: []
---

<objective>
Verify both renderer integrations work correctly with real backend streaming.

Purpose: Validates WIRE-01, WIRE-02, UX-01, UX-02 requirements through visual testing. Ensures both pages stream real responses and handle transient markup correctly.

Output: Verified working integration, ready for Phase 6 cleanup.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-renderer-integration/05-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Start development servers and verify connectivity</name>
  <files></files>
  <action>
1. Ensure backend is running (should already be on 188.245.108.179:8000)

2. Start frontend dev server:
   ```bash
   cd frontend && npm run dev
   ```

3. Verify backend connectivity:
   ```bash
   curl -X POST "http://188.245.108.179:8000/api/chat?marker=xml" \
     -H "Content-Type: application/json" \
     -d '{"messages":[{"role":"user","content":"Hello"}]}'
   ```

4. Verify both marker formats work:
   ```bash
   curl -X POST "http://188.245.108.179:8000/api/chat?marker=llm-ui" \
     -H "Content-Type: application/json" \
     -d '{"messages":[{"role":"user","content":"Hello"}]}'
   ```

If backend not running, start it:
   ```bash
   cd backend && python -m uvicorn main:app --host 0.0.0.0 --port 8000
   ```
  </action>
  <verify>
- Frontend runs on localhost:3000
- Backend responds to marker=xml requests
- Backend responds to marker=llm-ui requests
  </verify>
  <done>Development servers running, backend connectivity verified</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Both llm-ui and Streamdown pages wired to live backend with:
- Real agent responses (not mock data)
- Skeleton loaders for incomplete markup
- Stop button during streaming
- Error toast on failures
- Fade-in transition when components complete
  </what-built>
  <how-to-verify>
**Test llm-ui page (http://localhost:3000/llm-ui):**
1. Send "Tell me about John Smith's contact info" or use a preset
2. WATCH for: Skeleton appears briefly before ContactCard renders
3. VERIFY: No raw brackets [, ] visible during streaming
4. VERIFY: Component fades in smoothly when complete
5. Click Stop button during streaming - verify it stops
6. Check browser console for no errors

**Test Streamdown page (http://localhost:3000/streamdown):**
1. Send "Tell me about John Smith's contact info" or use a preset
2. WATCH for: Skeleton appears briefly before ContactCard renders
3. VERIFY: No raw XML tags visible during streaming
4. VERIFY: Component fades in smoothly when complete
5. Click Stop button during streaming - verify it stops
6. Check browser console for no errors

**Test error handling (both pages):**
1. Stop the backend server
2. Send a message
3. VERIFY: Toast notification appears with network error

**Test navigation cleanup:**
1. Start streaming on llm-ui page
2. Navigate to streamdown page before stream completes
3. VERIFY: No console warnings about state updates on unmounted component
  </how-to-verify>
  <resume-signal>Type "verified" if all checks pass, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
- [ ] llm-ui page streams real agent responses
- [ ] Streamdown page streams real agent responses
- [ ] No raw delimiters visible on llm-ui during streaming
- [ ] No raw XML tags visible on Streamdown during streaming
- [ ] Stop button works on both pages
- [ ] Error toasts appear when backend unavailable
- [ ] No console warnings on navigation during streaming
</verification>

<success_criteria>
1. llm-ui page fully functional with real backend (WIRE-01 complete)
2. Streamdown page fully functional with real backend (WIRE-02 complete)
3. No transient markup visible on llm-ui (UX-01 complete)
4. No transient markup visible on Streamdown (UX-02 complete)
5. All UX enhancements working (stop, error, fade)
</success_criteria>

<output>
After completion, create `.planning/phases/05-renderer-integration/05-04-SUMMARY.md`
</output>
