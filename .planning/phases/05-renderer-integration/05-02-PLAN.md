---
phase: 05-renderer-integration
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - frontend/app/llm-ui/page.tsx
  - frontend/components/llm-ui/LLMUIRenderer.tsx
autonomous: true

must_haves:
  truths:
    - "llm-ui page streams real responses from backend (not mock data)"
    - "No raw delimiters visible during streaming"
    - "User can stop streaming with stop button"
    - "Errors show toast notification"
    - "Stream aborted on page navigation"
  artifacts:
    - path: "frontend/app/llm-ui/page.tsx"
      provides: "llm-ui page with backend integration"
      contains: "marker=llm-ui"
    - path: "frontend/components/llm-ui/LLMUIRenderer.tsx"
      provides: "Renderer with skeleton support for incomplete blocks"
      contains: "ComponentSkeleton"
  key_links:
    - from: "frontend/app/llm-ui/page.tsx"
      to: "/api/chat?marker=llm-ui"
      via: "DefaultChatTransport"
      pattern: "marker=llm-ui"
    - from: "frontend/app/llm-ui/page.tsx"
      to: "sonner"
      via: "toast.error"
      pattern: "toast\\.error"
---

<objective>
Wire llm-ui page to live backend with marker=llm-ui, add stop button, error toast, and skeleton loading for incomplete blocks.

Purpose: Completes WIRE-01 (llm-ui backend wiring) and UX-01 (transient markup hiding). The page currently uses mock `/api/chat?format=llm-ui` - needs to switch to real backend with proper UX.

Output: llm-ui page fully integrated with backend, no visible raw delimiters during streaming.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-renderer-integration/05-CONTEXT.md
@.planning/phases/05-renderer-integration/05-RESEARCH.md

# Reference existing patterns
@frontend/app/flowtoken/page.tsx
@frontend/app/llm-ui/page.tsx
@frontend/components/llm-ui/LLMUIRenderer.tsx
@frontend/components/shared/StopButton.tsx
@frontend/components/shared/ComponentSkeleton.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire llm-ui page to backend with stop/error handling</name>
  <files>frontend/app/llm-ui/page.tsx</files>
  <action>
Update llm-ui/page.tsx following the FlowToken page pattern:

1. Change transport URL from mock to backend:
   - Add `const backendUrl = process.env.NEXT_PUBLIC_BACKEND_URL || 'http://188.245.108.179:8000';`
   - Change transport api from `/api/chat?format=llm-ui` to `${backendUrl}/api/chat?marker=llm-ui`

2. Add stop function and error handling:
   - Destructure `stop` from useChat
   - Add `import { toast } from 'sonner';`
   - Update onError to show toast:
     ```typescript
     onError: (err) => {
       console.error('[llm-ui] useChat onError:', err);
       const message = err.message.includes('fetch')
         ? 'Network error - check your connection'
         : err.message.includes('500')
         ? 'Server error - please try again'
         : 'An error occurred';
       toast.error(message);
     },
     ```

3. Add abort on unmount:
   ```typescript
   // Abort stream on unmount (prevents background streaming)
   useEffect(() => {
     return () => {
       stop();
     };
   }, [stop]);
   ```

4. Add StopButton to UI:
   - Import `StopButton` from '@/components/shared/StopButton'
   - Add `const isStreaming = status === 'streaming';`
   - Add StopButton between error display and TypingIndicator:
     ```tsx
     {isStreaming && (
       <div className="flex justify-center py-2">
         <StopButton onClick={stop} />
       </div>
     )}
     ```

5. Track last message for retry (optional enhancement):
   - Add `const [lastUserMessage, setLastUserMessage] = useState('');`
   - In handleSubmit, before sendMessage: `setLastUserMessage(message);`
   - In handlePresetSelect: `setLastUserMessage(message);`

Keep all other page logic unchanged (scroll handling, message formatting, etc.).
  </action>
  <verify>
- Transport URL contains `marker=llm-ui` (not `format=`)
- Uses environment variable for backend URL
- `stop` destructured from useChat
- toast import from sonner
- StopButton rendered conditionally on isStreaming
- useEffect cleanup calls stop()
  </verify>
  <done>llm-ui page wired to backend with stop button and error toast</done>
</task>

<task type="auto">
  <name>Task 2: Add skeleton loading for incomplete blocks in LLMUIRenderer</name>
  <files>frontend/components/llm-ui/LLMUIRenderer.tsx</files>
  <action>
Update LLMUIRenderer.tsx to show skeleton while blocks are incomplete:

1. Import ComponentSkeleton:
   ```typescript
   import { ComponentSkeleton } from '@/components/shared/ComponentSkeleton';
   ```

2. Track partial matches to show skeletons. Modify createBlockMatcher to handle partial state:

   In the component function, detect if we're rendering a partial match:
   ```typescript
   component: ({ blockMatch }: { blockMatch: BlockMatch }): ReactElement | null => {
     const rawOutput = blockMatch.outputRaw;

     // Check if this is a complete match (has closing delimiter)
     const isComplete = rawOutput.endsWith('】');

     // If incomplete, show skeleton
     if (!isComplete) {
       return <ComponentSkeleton type={blockType === 'CONTACT' ? 'contact' : 'calendar'} />;
     }

     // Extract JSON from complete block
     const jsonMatch = rawOutput.match(/【[A-Z]+:(\{[\s\S]*?\})】/);
     if (!jsonMatch) {
       return <span>{rawOutput}</span>;
     }
     // ... rest of existing parsing
   ```

   This requires passing the blockType to the component. Update the function signature and closure:
   ```typescript
   function createBlockMatcher<T extends ContactCardProps | CalendarEventProps>(
     blockType: 'CONTACT' | 'CALENDAR',  // More specific type
     BlockComponent: React.ComponentType<T>
   ): LLMOutputBlock {
   ```

3. Add fade-in class to completed components:
   ```typescript
   try {
     const props = JSON.parse(jsonMatch[1]) as T;
     return (
       <div className="component-fade-in">
         <BlockComponent {...props} />
       </div>
     );
   }
   ```

4. Update the lookBack function to return empty visibleText for both complete and incomplete (already does this, but verify):
   ```typescript
   lookBack: ({ output, isComplete }: LookBackFunctionParams) => {
     // Hide delimiters in both states
     return { output, visibleText: '' };
   },
   ```

The key insight: llm-ui calls the component with partial blockMatch during streaming. We detect incomplete state by checking if the output ends with the closing delimiter.
  </action>
  <verify>
- ComponentSkeleton imported
- createBlockMatcher checks for complete vs incomplete match
- Incomplete blocks render ComponentSkeleton
- Complete blocks wrapped in component-fade-in div
- No TypeScript errors: `npx tsc --noEmit`
  </verify>
  <done>LLMUIRenderer shows skeleton for incomplete blocks, fades in complete components</done>
</task>

</tasks>

<verification>
- [ ] Page transport uses `marker=llm-ui` with backend URL
- [ ] Stop button visible during streaming, calls stop()
- [ ] Error toast appears on network/server errors
- [ ] Stream aborted on page navigation (useEffect cleanup)
- [ ] Incomplete blocks show skeleton (not raw delimiters)
- [ ] Complete blocks fade in smoothly
- [ ] `npx tsc --noEmit` passes
</verification>

<success_criteria>
1. llm-ui page connects to real backend with marker=llm-ui (WIRE-01)
2. No raw delimiters [, ] visible during streaming (UX-01)
3. User can stop streaming via stop button
4. Errors show toast notification
5. Stream cleanup on navigation
</success_criteria>

<output>
After completion, create `.planning/phases/05-renderer-integration/05-02-SUMMARY.md`
</output>
